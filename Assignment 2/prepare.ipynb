{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required functions\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/SMSSpamCollection'\n",
    "def read_csv(path):\n",
    "    messages = pd.read_csv(path, sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "    print(\"No. of rows of data\",len(messages))\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows of data 5574\n"
     ]
    }
   ],
   "source": [
    "raw_data = read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split the words into tokens\n",
    "def split_into_tokens(data):\n",
    "    tokenized_words = []\n",
    "    regex=r\"\\w+\"\n",
    "    \n",
    "    for i in range(len(data.message)):\n",
    "        tokenized_words.append(re.findall(regex, data.message[i]))\n",
    "        \n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform lematization and stopword removal\n",
    "def lemmatize(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        temp = []\n",
    "\n",
    "        for j in range(len(data[i])):\n",
    "        \n",
    "            if data[i][j].lower() in stop_words:\n",
    "                continue\n",
    "            \n",
    "            elif data[i][j] in string.punctuation:\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                temp.append(str(lemmatizer.lemmatize(data[i][j]).lower()))\n",
    "\n",
    "        lemmatized_words.append(temp)             \n",
    "\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the required functions for pre-processing\n",
    "token_words = split_into_tokens(raw_data)\n",
    "processed_words = lemmatize(token_words)\n",
    "\n",
    "raw_data['processed_message'] = processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[raw_data.label == 'spam', 'Label'] = 1\n",
    "raw_data.loc[raw_data.label == 'ham', 'Label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split and save the train, validation and test split\n",
    "def data_split(data, r_seed):\n",
    "    \n",
    "    #Train-validation and test split\n",
    "    train_test_split_size = 0.1\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(data.processed_message, data.label, test_size = train_test_split_size, random_state = r_seed)\n",
    "\n",
    "    #Train and Validation split\n",
    "    train_val_split_size = 0.1\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = train_val_split_size, random_state = r_seed)\n",
    "\n",
    "    #Creating the splitted dataframes\n",
    "    train_df = pd.DataFrame({'X_train': X_train,'y_train': y_train})\n",
    "    val_df = pd.DataFrame({'X_val': X_val,'y_val': y_val})\n",
    "    test_df = pd.DataFrame({'X_test': X_test,'y_test': y_test})\n",
    "\n",
    "    train_df.to_csv('./data/train.csv',index = False)\n",
    "    val_df.to_csv('./data/validation.csv',index = False)\n",
    "    test_df.to_csv('./data/test.csv',index = False)\n",
    "    data.to_csv('./data/raw_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First data split\n",
    "data_split(raw_data, 42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tracking the data splitting using dvc**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "+---------------------------------------------------------------------+\n",
      "|                                                                     |\n",
      "|        DVC has enabled anonymous aggregate usage analytics.         |\n",
      "|     Read the analytics documentation (and how to opt-out) here:     |\n",
      "|             <https://dvc.org/doc/user-guide/analytics>              |\n",
      "|                                                                     |\n",
      "+---------------------------------------------------------------------+\n",
      "\n",
      "What's next?\n",
      "------------\n",
      "- Check out the documentation: <https://dvc.org/doc>\n",
      "- Get help and share ideas: <https://dvc.org/chat>\n",
      "- Star us on GitHub: <https://github.com/iterative/dvc>\n"
     ]
    }
   ],
   "source": [
    "!cd .. && dvc init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding all the 3 splitted csv files to dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add 'data\\.gitignore' train.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add 'data\\.gitignore' validation.csv.dvc\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "\tgit add test.csv.dvc 'data\\.gitignore'\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n"
     ]
    }
   ],
   "source": [
    "!dvc add ./data/train.csv\n",
    "!dvc add ./data/validation.csv\n",
    "!dvc add ./data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc config core.autostage true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding google drive folder as a remote data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 'myremote' as a default remote.\n"
     ]
    }
   ],
   "source": [
    "!cd .. && dvc remote add --default myremote gdrive://1WM_n-19W7nOe2Pyr-3yHzAUOFyvEgPOy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc remote modify myremote gdrive_acknowledge_abuse true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushing dvc tracked files to remote storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 files pushed\n"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd data split\n",
    "data_split(raw_data, 121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv.dvc:\n",
      "\tchanged outs:\n",
      "\t\tmodified:           Assignment 2\\data\\test.csv\n",
      "train.csv.dvc:\n",
      "\tchanged outs:\n",
      "\t\tmodified:           Assignment 2\\data\\train.csv\n",
      "validation.csv.dvc:\n",
      "\tchanged outs:\n",
      "\t\tmodified:           Assignment 2\\data\\validation.csv\n"
     ]
    }
   ],
   "source": [
    "!dvc status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 files pushed\n"
     ]
    }
   ],
   "source": [
    "!dvc push"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout for the different versions of the data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit b4e31b9f1fb6fd99a4f0f5eaee62322c65d0393f\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Tue Feb 28 00:16:57 2023 +0530\n",
      "\n",
      "    Second Split Random Seed 121\n",
      "\n",
      "commit 250db97be7ad21504f968454f6cf9cd55cad6bd1\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Tue Feb 28 00:07:45 2023 +0530\n",
      "\n",
      "    First Split Random Seed 42\n",
      "\n",
      "commit efe6d6c31d44abd0961ce32546915d7381ae2d97\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Tue Feb 28 00:07:15 2023 +0530\n",
      "\n",
      "    dvc remote added and pushed\n",
      "\n",
      "commit 2683509f5b8a4d5735d52e8e6ef0dba594985f9d\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Tue Feb 28 00:02:41 2023 +0530\n",
      "\n",
      "    dvc init\n",
      "\n",
      "commit 32d0797cf3e7baea5831db53520785188d28fe9a\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Tue Feb 28 00:01:44 2023 +0530\n",
      "\n",
      "    dvc deleted for fresh start\n",
      "\n",
      "commit a8cd5cea8129ba08b71b89fe78f42aa63563b417\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 23:59:51 2023 +0530\n",
      "\n",
      "    dvc deleted for fresh start\n",
      "\n",
      "commit fe80fa9a0d081b4e66077d57fa6f56041e7f2495\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 23:59:34 2023 +0530\n",
      "\n",
      "    dvc deleted for fresh start\n",
      "\n",
      "commit eb21fa349c494b1199a9431207c0ffc7242a10a7\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 22:33:21 2023 +0530\n",
      "\n",
      "    1st Split, Seed: 42\n",
      "\n",
      "commit 7e36fc00a177c12c7bcc4d1658353854f468cc19\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 22:32:51 2023 +0530\n",
      "\n",
      "    First Split not csv\n",
      "\n",
      "commit 2052aa111a482855597acce32630364e8386d41a\n",
      "Merge: e058d02 1793b56\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 21:46:45 2023 +0530\n",
      "\n",
      "    Moving files to subfolder\n",
      "\n",
      "commit e058d02378c703f9b2c244b11b022a6f55760816\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 21:45:13 2023 +0530\n",
      "\n",
      "    Moving files to subfolder\n",
      "\n",
      "commit 1793b56e78463d2c36d75f9c44ceda1c000b3113\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 21:43:48 2023 +0530\n",
      "\n",
      "    Delete data directory\n",
      "\n",
      "commit 98c78db52f7d7786d46bb1321b5a3c32bc059f17\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 21:43:02 2023 +0530\n",
      "\n",
      "    Moving files to subfolder\n",
      "\n",
      "commit 4edf56c2b0005cb04c944084a4f3909f24a71876\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 21:38:04 2023 +0530\n",
      "\n",
      "    Delete Assingment 1 directory\n",
      "\n",
      "commit c713be849d1edb0fd7208b21ee46b056539edda9\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 21:37:53 2023 +0530\n",
      "\n",
      "    Delete Assignment 1 directory\n",
      "\n",
      "commit 5b2ebaa8ab4ccd4f5ea1c5a1d647ed2a5cc28642\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 21:37:07 2023 +0530\n",
      "\n",
      "    Rename prepare.ipynb to Assignment 1/prepare.ipynb\n",
      "\n",
      "commit f8273b3140f443caa25a1c3c4a5c599b256615f5\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Mon Feb 27 21:33:26 2023 +0530\n",
      "\n",
      "    Create ReadMe.txt\n",
      "\n",
      "commit 44b1cd5733c28ee6b59a34b566ed17119eadf703\n",
      "Author: Soham Biswas <biswassoham434@gmail.com>\n",
      "Date:   Tue Jan 31 23:38:37 2023 +0530\n",
      "\n",
      "    initial_commit\n"
     ]
    }
   ],
   "source": [
    "!git log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout for 1st version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: switching to '250db97be7ad21504f968454f6cf9cd55cad6bd1'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 250db97 First Split Random Seed 42\n"
     ]
    }
   ],
   "source": [
    "!git checkout 250db97be7ad21504f968454f6cf9cd55cad6bd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M       Assignment 2\\data\\train.csv\n",
      "M       Assignment 2\\data\\test.csv\n",
      "M       Assignment 2\\data\\validation.csv\n"
     ]
    }
   ],
   "source": [
    "!dvc checkout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get the distribution of sms labels in the splitted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dist(path):\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    ham_count = list(data.iloc[:,1]).count(\"ham\")\n",
    "    spam_count = list(data.iloc[:,1]).count(\"spam\")\n",
    "\n",
    "    print(\"Ham: {}, Spam: {}\".format(ham_count, spam_count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1st Split (Random Seed: 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Split (Random Seed: 42)\n",
      "\n",
      "Training dataset:\n",
      "Ham: 3909, Spam: 605\n",
      "\n",
      "Validation dataset:\n",
      "Ham: 440, Spam: 62\n",
      "\n",
      "Testing dataset:\n",
      "Ham: 478, Spam: 80\n"
     ]
    }
   ],
   "source": [
    "train_path = './data/train.csv'\n",
    "val_path = './data/validation.csv'\n",
    "test_path = './data/test.csv'\n",
    "\n",
    "print(\"First Split (Random Seed: 42)\")\n",
    "print(\"\\nTraining dataset:\")\n",
    "label_dist(train_path)\n",
    "print(\"\\nValidation dataset:\")\n",
    "label_dist(val_path)\n",
    "print(\"\\nTesting dataset:\")\n",
    "label_dist(test_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout for 2nd Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Previous HEAD position was 250db97 First Split Random Seed 42\n",
      "HEAD is now at b4e31b9 Second Split Random Seed 121\n"
     ]
    }
   ],
   "source": [
    "!git checkout b4e31b9f1fb6fd99a4f0f5eaee62322c65d0393f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M       Assignment 2\\data\\train.csv\n",
      "M       Assignment 2\\data\\validation.csv\n",
      "M       Assignment 2\\data\\test.csv\n"
     ]
    }
   ],
   "source": [
    "!dvc checkout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2nd Split (Random Seed: 121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Split (Random Seed: 121)\n",
      "\n",
      "Training dataset:\n",
      "Ham: 3910, Spam: 604\n",
      "\n",
      "Validation dataset:\n",
      "Ham: 430, Spam: 72\n",
      "\n",
      "Testing dataset:\n",
      "Ham: 487, Spam: 71\n"
     ]
    }
   ],
   "source": [
    "print(\"2nd Split (Random Seed: 121)\")\n",
    "print(\"\\nTraining dataset:\")\n",
    "label_dist(train_path)\n",
    "print(\"\\nValidation dataset:\")\n",
    "label_dist(val_path)\n",
    "print(\"\\nTesting dataset:\")\n",
    "label_dist(test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9839ce3c6acaca560491e1b41f8b46d426659617553cc8a88a5826e3aaa30400"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
